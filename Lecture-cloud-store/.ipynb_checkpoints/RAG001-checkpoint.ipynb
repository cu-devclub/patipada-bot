{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b3363b0-260f-4198-8de0-87cfa0828720",
   "metadata": {
    "id": "6b3363b0-260f-4198-8de0-87cfa0828720"
   },
   "source": [
    "# หัวข้อ : สร้าง RAG สำหรับภาษาไทย ด้วย OpenSource\n",
    "\n",
    "#### จัดทำโดยทีม [VulturePrime](https://vultureprime.com)\n",
    "\n",
    "## รายละเอียด\n",
    "การสร้าง RAG เริ่มต้นเป็น Use case ที่แพร่หลายสำหรับการนำ AI เข้ามาประยุกต์ใช้กับธุรกิจ\n",
    "แต่เนื่องจากความขาดแคลนตัวอย่างที่เหมาะสมของแต่ละภาษา ทำให้เกิดปัญหา 2 ปัญหาได้แก่\n",
    "1. English centric ตัวอย่างส่วนใหญ่จะถูกทำขึ้นมาโดยใช้ภาษาอังกฤษเป็นศูนย์กลาง ทำให้ภาษาอื่นนั้น จำเป็นต้องประยุกต์ Solution ขึ้นมาเองซึ่งทำให้ Developer ต้องลงทุนในการเรียนรู้เพิ่มขึ้นอย่างมหาศาล ไม่ว่าจะเป็นในเรื่องของ Tokenizer หรือ Text splitter\n",
    "2. OpenAI centric เนื่องจากการเชื่อมต่อกับ OpenAI API นั้นเป็นเรื่องที่ใช้ Effort ต่ำสุดและมีประสิทธิภาพสูงที่สุด ทำให้ AI และ Embedding Model อื่นนั้น กลายเป็น 3rd class citizen เมื่อนักพัฒนาอยากลดรายจ่ายโดยใช้ AI และ Embedding รายอื่น จึงเป็นเรื่องที่มีต้นทุนสูงอย่างมหาศาล (Switching cost)  \n",
    "\n",
    "จากปัญหาทั้ง 2 ปัญหาทำให้เกิด AI Adoption ที่ช้ากว่าและแพงกว่าบริษัทที่ใช้ภาษาอังกฤษ​เป็นภาษาหลัก\n",
    "\n",
    "จึงเป็นที่มาของการสร้าง Use case ด้วยโปรเจคตัวอย่าง เพื่อให้ Developer สามารถลด Learning curve ลงไปได้\n",
    "และส่งเสริมให้มีความรวดเร็วในการนำ AI ไปใช้เพิ่มประสิทธิของ Feature ที่มีอยู่แล้วหรือสร้าง Feature ใหม่ขึ้นมา\n",
    "\n",
    "## บทความเพิ่มเติม (ภาษาไทย)\n",
    "- [Driving a Q&A Bot Project](https://www.vultureprime.com/blogs/driving-a-q-a-bot-project-a-product-owners-guide)\n",
    "\n",
    "## Benefit\n",
    "- ราคาต่อ Character ที่ถูกกว่า 90 - 95% เมื่อเทียบกับ OpenAI\n",
    "- เวลาในการประมวลผลที่น้อยลงเหลือเพียง 1/3 ถึง 1/4 (Float16 API หรือ SLA ตามต้องการ)\n",
    "- สามารถใช้งาน Offline หรือ Private เองได้\n",
    "\n",
    "## ความท้าทาย\n",
    "- การนับจำนวน Token ที่ไม่เท่ากันของภาษาอังกฤษและภาษาอื่น ทำให้ต้องสร้าง Custom Helper function เพื่อให้รองรับภาษาอื่น\n",
    "- การตัดประโยคของภาษาไทยเมื่อเทียบกับภาษาอื่น\n",
    "- การใช้งาน OpenSource เช่น Huggingface Embedding และ OpenAI API-like\n",
    "- การใช้งาน Vector Database และทำความเข้าใจ Parameter ในการค้นหา\n",
    "\n",
    "## Environment\n",
    "- [LlamaIndex](https://www.llamaindex.ai/) (Data Framework)\n",
    "- [Weaviate](https://weaviate.io/) (Vector Database)\n",
    "- [SeaLLM-7b](https://huggingface.co/SeaLLMs/SeaLLM-7B-Chat) (AI Model) ใช้งานผ่าน [Float16.cloud](https://float16.cloud)\n",
    "- [intfloat/multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) (Embedding Model)\n",
    "\n",
    "## Flow\n",
    "1. บันทึกข้อมูลรูปแบบ String Text ไปยัง Vector Database\n",
    "2. ค้นหาข้อมูลจาก Vector Database และนำไปใช้กับ AI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b20ba8-acba-475d-913c-d5ecfac38177",
   "metadata": {
    "id": "d3b20ba8-acba-475d-913c-d5ecfac38177"
   },
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466ef30-3bfb-4eb9-9aee-295b0f3ef623",
   "metadata": {
    "id": "7466ef30-3bfb-4eb9-9aee-295b0f3ef623",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1.เก็บข้อมูลรูปแบบ String Text ไปยัง Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa03704-cd15-41ee-befe-b8f6cabbc36b",
   "metadata": {
    "id": "aaa03704-cd15-41ee-befe-b8f6cabbc36b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ทำการ Import Library ที่จำเป็น\n",
    "from llama_index import ServiceContext, StorageContext,VectorStoreIndex,Document\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index.node_parser import TokenTextSplitter\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "#เชื่อมต่อกับ Vector Database\n",
    "auth_config = weaviate.AuthApiKey(api_key=\"IQQAPrPfigZjZI64pUdYwM08g8sXkZvcI9aO\")\n",
    "client = weaviate.Client(\n",
    "  url=\"https://monkbot001-6i46c5vt.weaviate.network\",\n",
    "  auth_client_secret=auth_config\n",
    ")\n",
    "\n",
    "#กำหนดชื่อ Document สำหรับ String Text ที่ต้องการบันทึกใน Vector Database\n",
    "document_name = 'Read_document'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf100132",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"read.txt\", \"r\", encoding=\"utf-8\") as Read_document:\n",
    "    Read_document = Read_document.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25ed829-5e57-4eb1-acea-08300bf56912",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "7e6c68815df147a594cda2dacec233b4",
      "4123b96127bd409fb1b406e1b842d027",
      "7deffd3448044e11a79d73a7a761fd1e",
      "bb64f7d3932d49ea97d104b4519c4d1f",
      "4003134500714b8981970839a3e8cdb1",
      "7f2f813672144271b88e04a287d73adc",
      "5bca17e98a4048a6ab0f54b8126f0b6b",
      "190f03616a1049edb8d13265bb424e85",
      "dd7a628bd4b04313a62670cbec672bc1",
      "fe7dcd8902db4e4ea1dc4c2c2cce4da2",
      "83a726cc00f442d1afe00dda16ef1d00"
     ]
    },
    "id": "e25ed829-5e57-4eb1-acea-08300bf56912",
    "outputId": "532d50f4-a672-48e8-837b-abc20d1cf458"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1349fb62734242e3b21f4bf5f7b6c4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#กำหนดค่าของ Text Splitter ที่ต้องการใช้\n",
    "#Chunk size กำหนดถึงความยาวของ Text ที่ต้องการตัด และ Chunk overlap หมายถึงการคาบเกี่ยวของ Text แต่ละชุดให้คาบเกี่ยวกับชุดก่อนหน้ามากน้อยแค่ไหน\n",
    "text_parser = TokenTextSplitter.from_defaults(chunk_overlap=192,chunk_size=384)\n",
    "\n",
    "#ประมวลผล String Text ให้กลายเป็น List[String] โดย String มีขนาดตามที่กำหนดจากขั้นตอนก่อนหน้านี้\n",
    "nodes = text_parser.get_nodes_from_documents(\n",
    "    [Document(text=Read_document)], show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e90e2206-f0ae-42c4-95ac-00b14b33e8fd",
   "metadata": {
    "id": "e90e2206-f0ae-42c4-95ac-00b14b33e8fd"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized model identifier in BAAI/bge-m3. Should contains one of 'bert', 'openai-gpt', 'gpt2', 'transfo-xl', 'xlnet', 'xlm', 'roberta, 'ctrl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msk-2dkHn98tNOxvFho3GidXT3BlbkFJ0JMZSbNTq0hVXGURZkOC\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#กำหนดชื่อ Embedding model ที่รองรับภาษาไทย\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#embedding_model_name = \"BAAI/bge-m3\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Download Embedding Model จาก Huggingface\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m HuggingFaceEmbedding(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBAAI/bge-m3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#กำหนดให้ใช้งาน Embedding ที่ทำการ Download มาจาก Huggingface\u001b[39;00m\n\u001b[0;32m     11\u001b[0m service_context \u001b[38;5;241m=\u001b[39m ServiceContext\u001b[38;5;241m.\u001b[39mfrom_defaults(embed_model\u001b[38;5;241m=\u001b[39membed_model)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\embeddings\\huggingface.py:82\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding.__init__\u001b[1;34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Use model_name with AutoModel\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     78\u001b[0m         model_name\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_HUGGINGFACE_EMBEDDING_MODEL\n\u001b[0;32m     81\u001b[0m     )\n\u001b[1;32m---> 82\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     83\u001b[0m         model_name, cache_dir\u001b[38;5;241m=\u001b[39mcache_folder, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code\n\u001b[0;32m     84\u001b[0m     )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Extract model_name from model\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mname_or_path\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_auto.py:157\u001b[0m, in \u001b[0;36mAutoModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctrl\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pretrained_model_name_or_path:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CTRLModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized model identifier in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Should contains one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai-gpt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransfo-xl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlnet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxlm\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mctrl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pretrained_model_name_or_path))\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized model identifier in BAAI/bge-m3. Should contains one of 'bert', 'openai-gpt', 'gpt2', 'transfo-xl', 'xlnet', 'xlm', 'roberta, 'ctrl'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = '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'\n",
    "\n",
    "#กำหนดชื่อ Embedding model ที่รองรับภาษาไทย\n",
    "#embedding_model_name = \"BAAI/bge-m3\"\n",
    "\n",
    "#Download Embedding Model จาก Huggingface\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "\n",
    "#กำหนดให้ใช้งาน Embedding ที่ทำการ Download มาจาก Huggingface\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n",
    "\n",
    "#กำหนดชื่อสำหรับ text key สำหรับการค้นหา\n",
    "text_key = 'content'\n",
    "\n",
    "#สร้าง VectorStore Node\n",
    "vector_store = WeaviateVectorStore(weaviate_client = client,index_name = document_name,text_key = text_key)\n",
    "\n",
    "#กำหนดให้ใช้งาน VectorDatabase ที่ทำการเชื่อมต่อกับ Weaviate\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fd9a50a-6a69-4fb5-834a-f3e5775fec8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fd9a50a-6a69-4fb5-834a-f3e5775fec8f",
    "outputId": "9e55403c-fda7-41d9-93a1-3a72310daf14"
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#นำ List[String] ที่ได้จากขั้นตอนประมวลผล Text นำมาแปลงให้อยู่ในรูปของ List[Vector]\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#แปลง String เป็น Vector โดยใช้ Embedding Model intfloat/multilingual-e5-small\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#หลังจากได้ List[Vector] และ List[String] ให้นำข้อมูลดังกล่าวเก็บไว้ยัง VectorDatabase ที่ได้เชื่อมต่อไว้แล้ว\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#โดยเก็บไว้ใน Document ชื่อ Read_document และมี Text_key เริ่มต้นด้วย \"content\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m VectorStoreIndex(nodes\u001b[38;5;241m=\u001b[39mnodes,storage_context\u001b[38;5;241m=\u001b[39mstorage_context, service_context \u001b[38;5;241m=\u001b[39m service_context)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:52\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, index_struct, service_context, storage_context, use_async, store_nodes_override, insert_batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override \u001b[38;5;241m=\u001b[39m store_nodes_override\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     53\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     54\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     55\u001b[0m     service_context\u001b[38;5;241m=\u001b[39mservice_context,\n\u001b[0;32m     56\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m     57\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     59\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\indices\\base.py:72\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[1;34m(self, nodes, index_struct, storage_context, service_context, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_index_from_nodes(nodes)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:271\u001b[0m, in \u001b[0;36mVectorStoreIndex.build_index_from_nodes\u001b[1;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    264\u001b[0m     node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[0;32m    265\u001b[0m ):\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot build index from nodes with no content. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure all nodes have content.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m     )\n\u001b[1;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_index_from_nodes(nodes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:243\u001b[0m, in \u001b[0;36mVectorStoreIndex._build_index_from_nodes\u001b[1;34m(self, nodes, **insert_kwargs)\u001b[0m\n\u001b[0;32m    241\u001b[0m     run_async_tasks(tasks)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_nodes_to_index(\n\u001b[0;32m    244\u001b[0m         index_struct,\n\u001b[0;32m    245\u001b[0m         nodes,\n\u001b[0;32m    246\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_progress,\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs,\n\u001b[0;32m    248\u001b[0m     )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:196\u001b[0m, in \u001b[0;36mVectorStoreIndex._add_nodes_to_index\u001b[1;34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size):\n\u001b[1;32m--> 196\u001b[0m     nodes_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_node_with_embedding(nodes_batch, show_progress)\n\u001b[0;32m    197\u001b[0m     new_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39madd(nodes_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minsert_kwargs)\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_store\u001b[38;5;241m.\u001b[39mstores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_nodes_override:\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\indices\\vector_store\\base.py:104\u001b[0m, in \u001b[0;36mVectorStoreIndex._get_node_with_embedding\u001b[1;34m(self, nodes, show_progress)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_node_with_embedding\u001b[39m(\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     95\u001b[0m     nodes: Sequence[BaseNode],\n\u001b[0;32m     96\u001b[0m     show_progress: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseNode]:\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    Allows us to store these nodes in a vector store.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    Embeddings are called in batches.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     id_to_embed_map \u001b[38;5;241m=\u001b[39m embed_nodes(\n\u001b[0;32m    105\u001b[0m         nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_context\u001b[38;5;241m.\u001b[39membed_model, show_progress\u001b[38;5;241m=\u001b[39mshow_progress\n\u001b[0;32m    106\u001b[0m     )\n\u001b[0;32m    108\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\indices\\utils.py:137\u001b[0m, in \u001b[0;36membed_nodes\u001b[1;34m(nodes, embed_model, show_progress)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m         id_to_embed_map[node\u001b[38;5;241m.\u001b[39mnode_id] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39membedding\n\u001b[1;32m--> 137\u001b[0m new_embeddings \u001b[38;5;241m=\u001b[39m embed_model\u001b[38;5;241m.\u001b[39mget_text_embedding_batch(\n\u001b[0;32m    138\u001b[0m     texts_to_embed, show_progress\u001b[38;5;241m=\u001b[39mshow_progress\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[0;32m    142\u001b[0m     id_to_embed_map[new_id] \u001b[38;5;241m=\u001b[39m text_embedding\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\core\\embeddings\\base.py:256\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding_batch\u001b[1;34m(self, texts, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cur_batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_batch_size:\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;66;03m# flush\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[0;32m    253\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING,\n\u001b[0;32m    254\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()},\n\u001b[0;32m    255\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[1;32m--> 256\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_text_embeddings(cur_batch)\n\u001b[0;32m    257\u001b[0m         result_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[0;32m    258\u001b[0m         event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[0;32m    259\u001b[0m             payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    260\u001b[0m                 EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: cur_batch,\n\u001b[0;32m    261\u001b[0m                 EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: embeddings,\n\u001b[0;32m    262\u001b[0m             },\n\u001b[0;32m    263\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\embeddings\\openai.py:386\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_text_embeddings\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get text embeddings.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03mBy default, this is a wrapper around _get_text_embedding.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03mCan be overridden for batch queries.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    385\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_client()\n\u001b[1;32m--> 386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_embeddings(\n\u001b[0;32m    387\u001b[0m     client,\n\u001b[0;32m    388\u001b[0m     texts,\n\u001b[0;32m    389\u001b[0m     engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_engine,\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs,\n\u001b[0;32m    391\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:325\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    323\u001b[0m     retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:158\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\llama_index\\embeddings\\openai.py:162\u001b[0m, in \u001b[0;36mget_embeddings\u001b[1;34m(client, list_of_text, engine, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe batch size should not be larger than 2048.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m list_of_text \u001b[38;5;241m=\u001b[39m [text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[1;32m--> 162\u001b[0m data \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mlist_of_text, model\u001b[38;5;241m=\u001b[39mengine, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [d\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\embeddings.py:103\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m     97\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m     98\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    105\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[0;32m    106\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    107\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[0;32m    108\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[0;32m    109\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[0;32m    110\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    111\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    112\u001b[0m     ),\n\u001b[0;32m    113\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[0;32m    114\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1112\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1100\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1107\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1108\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1109\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1110\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1111\u001b[0m     )\n\u001b[1;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:859\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    852\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    860\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    861\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    862\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    863\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    864\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    865\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:934\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    933\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    935\u001b[0m         options,\n\u001b[0;32m    936\u001b[0m         cast_to,\n\u001b[0;32m    937\u001b[0m         retries,\n\u001b[0;32m    938\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    939\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    940\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    941\u001b[0m     )\n\u001b[0;32m    943\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    980\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    983\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    984\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    985\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m    986\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    988\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:934\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    933\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    935\u001b[0m         options,\n\u001b[0;32m    936\u001b[0m         cast_to,\n\u001b[0;32m    937\u001b[0m         retries,\n\u001b[0;32m    938\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    939\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    940\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    941\u001b[0m     )\n\u001b[0;32m    943\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    980\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    983\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    984\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    985\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m    986\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    988\u001b[0m )\n",
      "    \u001b[1;31m[... skipping similar frames: SyncAPIClient._request at line 934 (7 times), SyncAPIClient._retry_request at line 982 (7 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:934\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m    933\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m    935\u001b[0m         options,\n\u001b[0;32m    936\u001b[0m         cast_to,\n\u001b[0;32m    937\u001b[0m         retries,\n\u001b[0;32m    938\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    939\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    940\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    941\u001b[0m     )\n\u001b[0;32m    943\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m    980\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m--> 982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    983\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    984\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    985\u001b[0m     remaining_retries\u001b[38;5;241m=\u001b[39mremaining,\n\u001b[0;32m    986\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    988\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:949\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    946\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    948\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 949\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    952\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    953\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    957\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "#นำ List[String] ที่ได้จากขั้นตอนประมวลผล Text นำมาแปลงให้อยู่ในรูปของ List[Vector]\n",
    "#แปลง String เป็น Vector โดยใช้ Embedding Model intfloat/multilingual-e5-small\n",
    "#หลังจากได้ List[Vector] และ List[String] ให้นำข้อมูลดังกล่าวเก็บไว้ยัง VectorDatabase ที่ได้เชื่อมต่อไว้แล้ว\n",
    "#โดยเก็บไว้ใน Document ชื่อ Read_document และมี Text_key เริ่มต้นด้วย \"content\"\n",
    "VectorStoreIndex(nodes=nodes,storage_context=storage_context, service_context = service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362962d0-b817-4a59-aeef-083ec5963c0d",
   "metadata": {
    "id": "362962d0-b817-4a59-aeef-083ec5963c0d"
   },
   "source": [
    "#### 2. ค้นหาข้อมูลจาก Vector Database และนำไปใช้กับ AI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d230f3f3-b632-4974-9036-7b529d9bb894",
   "metadata": {
    "id": "d230f3f3-b632-4974-9036-7b529d9bb894"
   },
   "outputs": [],
   "source": [
    "#ทำการ Import Library ที่จำเป็น\n",
    "from llama_index.llms import openai_like\n",
    "from llama_index.readers.weaviate.reader import WeaviateReader\n",
    "from llama_index import ServiceContext, ListIndex\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.indices.prompt_helper import PromptHelper\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6375705a-518d-4421-9cfe-bd9a2a6fb418",
   "metadata": {
    "id": "6375705a-518d-4421-9cfe-bd9a2a6fb418"
   },
   "outputs": [],
   "source": [
    "#ทำการ Config API ที่ต้องการใช้\n",
    "OPENAI_KEY = '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'\n",
    "FLOAT16_API_KEY = 'float16-56hBpTpGA5VYSs13Awe9U1FjGaTo5pNODx67cnI066EBP5rewr'\n",
    "FLOAT16_CUSTOM_URL = 'https://api.float16.cloud/v1/llamaindex'\n",
    "WEAVIATE_ENDPOINT = \"https://munkbot-zzx9jngn.weaviate.network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27500c47-0b1b-479a-8388-5c627850f4ce",
   "metadata": {
    "id": "27500c47-0b1b-479a-8388-5c627850f4ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geevh\\anaconda3\\Lib\\site-packages\\weaviate\\warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.0. The latest version is 4.4.4.\n",
      "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#เชื่อมต่อกับ VectorDatabase\n",
    "auth_config = weaviate.AuthApiKey(api_key=\"YBS4VORSc2Fc4EvY3zM0FHTl56mdvuhgJRKR\")\n",
    "\n",
    "client = weaviate.Client(\n",
    "  url=\"https://munkbot-zzx9jngn.weaviate.network\",\n",
    "  auth_client_secret=auth_config\n",
    ")\n",
    "\n",
    "reader = WeaviateReader(\n",
    "    \"https://munkbot-zzx9jngn.weaviate.network\",\n",
    "    auth_client_secret=auth_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cfcf059-3b3b-4427-a05d-53e9a5dc4fdf",
   "metadata": {
    "id": "3cfcf059-3b3b-4427-a05d-53e9a5dc4fdf"
   },
   "outputs": [],
   "source": [
    "#สร้าง GraphQL query schema สำหรับการค้นหาข้อมูลที่เกี่ยวข้องจาก VectorDatabase\n",
    "query_schema = \"\"\"\n",
    "{{\n",
    "  Get{{\n",
    "    {document_name}(\n",
    "      nearVector: {{\n",
    "        vector: {vector_question},\n",
    "        certainty: {certainty}\n",
    "      }}\n",
    "      limit : {limit}\n",
    "    ) {{\n",
    "      {text_key}\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55dd6a73-968b-4c7b-bf35-6ecae0bd9ac0",
   "metadata": {
    "id": "55dd6a73-968b-4c7b-bf35-6ecae0bd9ac0"
   },
   "outputs": [],
   "source": [
    "#กำหนดชื่อ Embedding model ที่รองรับภาษาไทย\n",
    "embedding_model_name = \"intfloat/multilingual-e5-small\"\n",
    "\n",
    "#Download Embedding Model จาก Huggingface\n",
    "embed_model = HuggingFaceEmbedding(model_name=embedding_model_name,max_length=384)\n",
    "\n",
    "#ระบุคำถามที่ต้องการถาม AI\n",
    "question = \"อุเทสิกเจดีย์คือ\"\n",
    "\n",
    "#เปลี่ยนคำถาม (String) ให้กลายเป็น Vector\n",
    "vector_question = embed_model._embed(question)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2e4eb71-bab7-4be2-83ca-2c3e19c87296",
   "metadata": {
    "id": "c2e4eb71-bab7-4be2-83ca-2c3e19c87296"
   },
   "outputs": [],
   "source": [
    "#กำหนดชื่อ Document สำหรับ String Text ที่ต้องการบันทึกใน Vector Database\n",
    "document_name = 'Read_document'\n",
    "\n",
    "#กำหนดชื่อสำหรับ text key สำหรับการค้นหา\n",
    "text_key = 'content'\n",
    "\n",
    "#ทำการ map ตัวแปรเข้ากับ Schema\n",
    "query = query_schema.format(\n",
    "  document_name=document_name,\n",
    "  vector_question=vector_question,\n",
    "  certainty=0.96,\n",
    "  text_key=text_key,\n",
    "  limit=1\n",
    ")\n",
    "\n",
    "#ส่งคำสั่ง query ไปยัง VectorDatabase\n",
    "documents = reader.load_data(graphql_query=query, separate_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04f6b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id_='d16ddc4a-c341-47a6-a472-a0065e77c9bb', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='7567acd3fc7df69aeee7fd0b86bfd61d491c47dcd3e411ff60799f3f1b4c52ae', text='content: เครื่องสักการะเครื่องบูชาใครไปนั่งตรงนั้นก็ได้รับการสักการะบูชา เพราะว่าผู้แสดงธรรมเป็นผู้ที่ควรแก่สักการะบูชา ที่เรียกว่าอุเทสิกเจดีย์เค้าเรียกว่าเป็นสิ่งที่ควรค่า ไม่ได้ว่าจะเป็นเฉพาะพระภิกษุนะฆราวาสเองก็ตาม ถ้าเขาจัดตั้งการบูชาไว้ก็คือต้องการบูชาอุเทสิกเจดีย์\\nอุเทสิกเจดีย์ตัวนี้ไม่มีวัตถุปรากฏ แต่เป็นสิ่งที่เนื่องด้วยพระตถาคต คำว่าสิ่งที่เนื่องด้วยพระตถาคตคือการแสดงธรรม เพราะอุเทสิกมีความหมายว่าการยกขึ้น', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n"
     ]
    }
   ],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1e1571-2768-4c90-b808-9aa81aa0c9b3",
   "metadata": {
    "id": "0c1e1571-2768-4c90-b808-9aa81aa0c9b3"
   },
   "source": [
    "#### กำหนด Service ที่เกี่ยวข้องกับ AI\n",
    "\n",
    "llm คือการกำหนดให้ใช้งาน LLM ตัวใดหรือผู้ให้บริการเจ้าใดสำหรับการรับ Prompt เพื่อให้ตอบคำถาม\n",
    "\n",
    "embed_model คือการกำหนดให้ใช้งาน Embedding ตัวใด สำหรับการแปลง Text ให้เป็น Vector\n",
    "\n",
    "prompt_helper คือตัวช่วยจิปาถะที่เกี่ยวข้องกับการใช้งาน LLM เช่น การทำให้ Prompt ที่ส่งไปยังผู้ให้บริการมีขนาดไม่เกินที่กำหนด (max_context)\n",
    "\n",
    "หรือ เมื่อ Prompt มีขนาดยาวเกินไปก็จะทำการ \"ตัดข้อมูล\" โดยอัตโนมัติ\n",
    "\n",
    "สำหรับ context_window ที่เรากำหนดขึ้นมานั้นมีขนาด 10,000 token ซึ่งการนับ Token โดย Default นั้น ใช้งาน Tiktoken หรือ OpenAI ในการนับ Token\n",
    "\n",
    "ทำให้ไม่สอดคล้องกับ SeaLLM-7b เนื่องจาก SeaLLM-7b นับ Token ต่างกับ OpenAI\n",
    "\n",
    "ส่งผลให้เมื่อใช้งาน context_window default จะทำให้ข้อมูลขาดหายไปบางช่วง\n",
    "\n",
    "DEFAULT context_window = 3900 #token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6487c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ระบุ model ที่ต้องการใช้งาน\n",
    "llm_model_name = \"seallm-7b-v2\"\n",
    "\n",
    "#เชื่อมต่อกับ Float16\n",
    "agent = openai_like.OpenAILike(\n",
    "    api_key=FLOAT16_API_KEY,\n",
    "    api_base=FLOAT16_CUSTOM_URL,\n",
    "    model=llm_model_name,\n",
    "    temperature=0.3,\n",
    "    max_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0f365cf-9f49-415b-8c47-448c73f1b65f",
   "metadata": {
    "id": "a0f365cf-9f49-415b-8c47-448c73f1b65f"
   },
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(llm=agent,embed_model=embed_model,prompt_helper=PromptHelper(context_window=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6738a7c-9dd9-4c33-8a2a-13f0ff83796f",
   "metadata": {
    "id": "a6738a7c-9dd9-4c33-8a2a-13f0ff83796f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ListIndex' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#สร้าง index สำหรับการค้นหาข้อมูลจาก ผลลัพธ์จาก query\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m index \u001b[38;5;241m=\u001b[39m ListIndex\u001b[38;5;241m.\u001b[39mfrom_documents(documents, service_context\u001b[38;5;241m=\u001b[39mservice_context)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ListIndex' is not defined"
     ]
    }
   ],
   "source": [
    "#สร้าง index สำหรับการค้นหาข้อมูลจาก ผลลัพธ์จาก query\n",
    "index = ListIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b69955-9f99-487c-acd2-214e15f19ac2",
   "metadata": {
    "id": "b7b69955-9f99-487c-acd2-214e15f19ac2"
   },
   "source": [
    "#### กำหนด Prompt pipeline สำหรับ RAG\n",
    "\n",
    "Default สำหรับ Llamaindex จะเป็นการ \"refine\" ซึ่ง refine จะเป็นขั้นตอนการทำงาน 2 ขั้นตอนต่อกัน\n",
    "1. Question and Answer เพื่อให้ LLM ที่กำหนดไว้ใน Service_context ตอบคำถาม\n",
    "2. Refine เพื่อให้คำตอบที่ได้ออกมานั้นสวยงามมากยิ่งขึ้น\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "989bce91-eaf7-4945-b813-77eb3fca99e9",
   "metadata": {
    "id": "989bce91-eaf7-4945-b813-77eb3fca99e9"
   },
   "outputs": [],
   "source": [
    "#กำหนด Prompt pipeline สำหรับ RAG\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "      response_mode=\"simple_summarize\",service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e089a97-92c8-40c6-9ec1-943ec8db15a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e089a97-92c8-40c6-9ec1-943ec8db15a3",
    "outputId": "9cbf647c-32b5-4cd7-d29e-7ee2d12ca6ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: เครื่องสักการะเครื่องบูชาใครไปนั่งตรงนั้นก็ได้รับการสักการะบูชา เพราะว่าผู้แสดงธรรมเป็นผู้ที่ควรแก่สักการะบูชา ที่เรียกว่าอุเทสิกเจดีย์เค้าเรียกว่าเป็นสิ่งที่ควรค่า ไม่ได้ว่าจะเป็นเฉพาะพระภิกษุนะฆราวาสเองก็ตาม ถ้าเขาจัดตั้งการบูชาไว้ก็คือต้องการบูชาอุเทสิกเจดีย์\n",
      "อุเทสิกเจดีย์ตัวนี้ไม่มีวัตถุปรากฏ แต่เป็นสิ่งที่เนื่องด้วยพระตถาคต คำว่าสิ่งที่เนื่องด้วยพระตถาคตคือการแสดงธรรม เพราะอุเทสิกมีความหมายว่าการยกขึ้น\n",
      "result : อุเทสิกเจดีย์คือสิ่งที่เนื่องด้วยพระตถาคต คำว่าสิ่งที่เนื่องด้วยพระตถาคตคือการแสดงธรรม เพราะอุเทสิกมีความหมายว่าการยกขึ้น\n"
     ]
    }
   ],
   "source": [
    "#สร้าง Query engine จาก index ที่เราได้สร้างจาก ผลลัพธ์จาก Query และเปลี่ยน Response ให้ทำงานแค่ 1 ขั้นตอน\n",
    "query_engine = index.as_query_engine(\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    service_context=service_context,\n",
    ")\n",
    "\n",
    "#ค้นหาผลลัพธ์จาก RAG\n",
    "result = query_engine.query(\"<summarization_query>\")\n",
    "print(documents[0].text)\n",
    "print(\"result :\",result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e752fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "อุเทสิกเจดีย์คือสิ่งที่เนื่องด้วยพระตถาคต ซึ่งเป็นสิ่งที่เนื่องด้วยการแสดงธรรมหรือการยกขึ้นในความหมายของอุเทสิก. มันคือสิ่งที่ควรค่าและเป็นสิ่งที่ควรได้รับการบูชา. ผู้แสดงธรรมเป็นผู้ที่ควรได้รับการบูชาอุเทสิกเจดีย์ และการจัดตั้งการบูชานั้นแสดงถึงความต้องการบูชาอุเทสิกเจดีย์.\n"
     ]
    }
   ],
   "source": [
    "#สร้าง Query engine จาก index ที่เราได้สร้างจาก ผลลัพธ์จาก Query และเปลี่ยน Response ให้ทำงานแค่ 1 ขั้นตอน\n",
    "query_engine = index.as_query_engine(\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    service_context=service_context,\n",
    ")\n",
    "\n",
    "#ค้นหาผลลัพธ์จาก RAG\n",
    "result = query_engine.query(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b914ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "190f03616a1049edb8d13265bb424e85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4003134500714b8981970839a3e8cdb1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4123b96127bd409fb1b406e1b842d027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f2f813672144271b88e04a287d73adc",
      "placeholder": "​",
      "style": "IPY_MODEL_5bca17e98a4048a6ab0f54b8126f0b6b",
      "value": "Parsing nodes: 100%"
     }
    },
    "5bca17e98a4048a6ab0f54b8126f0b6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7deffd3448044e11a79d73a7a761fd1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_190f03616a1049edb8d13265bb424e85",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dd7a628bd4b04313a62670cbec672bc1",
      "value": 1
     }
    },
    "7e6c68815df147a594cda2dacec233b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4123b96127bd409fb1b406e1b842d027",
       "IPY_MODEL_7deffd3448044e11a79d73a7a761fd1e",
       "IPY_MODEL_bb64f7d3932d49ea97d104b4519c4d1f"
      ],
      "layout": "IPY_MODEL_4003134500714b8981970839a3e8cdb1"
     }
    },
    "7f2f813672144271b88e04a287d73adc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83a726cc00f442d1afe00dda16ef1d00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb64f7d3932d49ea97d104b4519c4d1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe7dcd8902db4e4ea1dc4c2c2cce4da2",
      "placeholder": "​",
      "style": "IPY_MODEL_83a726cc00f442d1afe00dda16ef1d00",
      "value": " 1/1 [00:00&lt;00:00, 11.71it/s]"
     }
    },
    "dd7a628bd4b04313a62670cbec672bc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe7dcd8902db4e4ea1dc4c2c2cce4da2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
